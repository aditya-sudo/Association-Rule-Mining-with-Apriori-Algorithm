{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVWQYpEMQWmi"
      },
      "source": [
        "I am going to examine movies using our understanding of association rules. For this part, I implemented the apriori algorithm, and apply it to a movie rating dataset to find association rules of user-rate-movie behaviors. First, run the next cell to load the dataset we are going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3vLhbqNQWmk",
        "outputId": "30a16aaa-5ef1-49fb-cc8f-7864edaa9b07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "array of user-movie matrix: shape (11743, 2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "user_movie_data = np.loadtxt(\"movie_rated.txt\", delimiter=',')\n",
        "print('array of user-movie matrix: shape ' + str(np.shape(user_movie_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mipzW6dNQWmp"
      },
      "source": [
        "In this dataset, there are two columns: the first column is the integer ids of users, and the second column is the integer ids of movies. Each row denotes that the user of given user id watched the movie of the given movie id. We are going to treat each user as a transaction, so you will need to collect all the movies that have been watched by a single user as a transaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdmL17CwQWnA"
      },
      "source": [
        "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user movie-watching behaviors with **minimum support of 0.2** and **minimum confidence of 0.8**. We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach.\n",
        "\n",
        "**Note: Do not copy-paste any existing code.**\n",
        "\n",
        "**Note: We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing.**\n",
        "\n",
        "**Note: You should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will explain the process of the method.**\n",
        "\n",
        "**Hint: If you implement your algorithm correctly, you should be able to see intermediate result as:**\n",
        "- Candidates of length 1 count: 408\n",
        "- After Pruning count: 21\n",
        "- Candidates of length 2 count: 210\n",
        "- After Pruning 2 count: 36\n",
        "- Candidates of length 3 count: 55\n",
        "- After Pruning 3 count: 12\n",
        "- Candidates of length 4 count: 1\n",
        "- After Pruning 4 count: 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJSeoBzyQWnN",
        "outputId": "b3916598-adea-4425-c778-8d65417b0305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidates of length 1 count: 408\n",
            "After Pruning 1 count: 21\n",
            "Candidates of length 2 count: 210\n",
            "After Pruning 2 count: 36\n",
            "Candidates of length 3 count: 123\n",
            "After Pruning 3 count: 12\n",
            "Candidates of length 4 count: 15\n",
            "After Pruning 4 count: 0\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "\n",
        "# Load the movies.csv file\n",
        "movies = pd.read_csv('movies.csv')\n",
        "\n",
        "# Function to fetch movie names based on movie ids\n",
        "def get_movie_names(movie_ids):\n",
        "    \"\"\"\n",
        "    Fetches the movie names from the movies.csv file based on the given movie ids.\n",
        "\n",
        "    Args:\n",
        "        movie_ids (set): A set of movie ids.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of movie names.\n",
        "    \"\"\"\n",
        "    movie_names = []\n",
        "    for movie_id in movie_ids:\n",
        "        movie = movies[movies['movieId'] == movie_id]\n",
        "        if not movie.empty:\n",
        "            movie_names.append(movie['movie_name'].values[0])\n",
        "    return movie_names\n",
        "\n",
        "# Preprocess data into transactions\n",
        "transactions = defaultdict(set)\n",
        "for user_id, movie_id in user_movie_data:\n",
        "    transactions[user_id].add(movie_id)\n",
        "\n",
        "transactions = list(transactions.values())\n",
        "\n",
        "# Function to calculate support\n",
        "def calculate_support(candidate, transactions):\n",
        "    \"\"\"\n",
        "    Calculates the support for a given candidate itemset.\n",
        "\n",
        "    Args:\n",
        "        candidate (frozenset): The candidate itemset.\n",
        "        transactions (list): The list of transactions.\n",
        "\n",
        "    Returns:\n",
        "        float: The support for the candidate itemset.\n",
        "    \"\"\"\n",
        "    count = sum(1 for transaction in transactions if candidate.issubset(transaction))\n",
        "    return count / len(transactions)\n",
        "\n",
        "# Function to generate candidates of a given length\n",
        "def generate_candidates(prev_candidates, length):\n",
        "    \"\"\"\n",
        "    Generates the candidate itemsets of a given length based on the previous frequent itemsets.\n",
        "\n",
        "    Args:\n",
        "        prev_candidates (set): The previous frequent itemsets.\n",
        "        length (int): The desired length of the candidate itemsets.\n",
        "\n",
        "    Returns:\n",
        "        set: The set of candidate itemsets of the given length.\n",
        "    \"\"\"\n",
        "    return set([frozenset(i.union(j)) for i in prev_candidates for j in prev_candidates if len(i.union(j)) == length])\n",
        "\n",
        "# Apriori algorithm\n",
        "def apriori(transactions, min_support, min_confidence):\n",
        "    \"\"\"\n",
        "    Implements the Apriori algorithm to find frequent itemsets.\n",
        "\n",
        "    Args:\n",
        "        transactions (list): The list of transactions.\n",
        "        min_support (float): The minimum support threshold.\n",
        "        min_confidence (float): The minimum confidence threshold.\n",
        "\n",
        "    Returns:\n",
        "        list: The list of frequent itemsets.\n",
        "    \"\"\"\n",
        "    # Initial candidate set (C1)\n",
        "    candidates = set(frozenset([m]) for transaction in transactions for m in transaction)\n",
        "    print(f\"Candidates of length 1 count: {len(candidates)}\")\n",
        "\n",
        "    # Store frequent itemsets\n",
        "    frequent_itemsets = []\n",
        "\n",
        "    k = 2\n",
        "    while candidates:\n",
        "        # Calculate support for candidates and filter by min_support\n",
        "        candidate_support = {c: calculate_support(c, transactions) for c in candidates}\n",
        "        candidates = set(c for c, s in candidate_support.items() if s >= min_support)\n",
        "        print(f\"After Pruning {k-1} count: {len(candidates)}\")\n",
        "\n",
        "        if candidates:\n",
        "            frequent_itemsets.extend(candidates)\n",
        "            candidates = generate_candidates(candidates, k)\n",
        "            print(f\"Candidates of length {k} count: {len(candidates)}\")\n",
        "            k += 1\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "    # Apply Apriori algorithm\n",
        "frequent_itemsets = apriori(transactions, min_support=0.2, min_confidence=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z9Zxj2oQWnQ"
      },
      "source": [
        "Finally, print your final association rules in the following format:\n",
        "\n",
        "**movie_name_1, movie_name_2, ... --> movie_name_k**\n",
        "\n",
        "where the movie names can be fetched by joining the movieId with the file 'movies.csv'. For example, one rule that you should find is:\n",
        "\n",
        "**Jurassic Park (1993), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)**\n",
        "\n",
        "**Hint: if you implement the algorith correctly, you will find 14 rules in total.**\n",
        "\n",
        "**Hint: You may need to use the Pandas library to load and process the movies.csv file, such as use pandas.read_csv() to load the data. https://pandas.pydata.org/pandas-docs/dev/user_guide/10min.html is a good place to learn the basics about Pandas.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu3foArLQWnU",
        "outputId": "2c226d5e-f40e-484b-fedb-9e93cd09cc02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rule: ['Godfather: Part II, The (1974)'] -> ['Godfather, The (1972)'], Support: 0.2388663967611336, Confidence: 0.8489208633093526\n",
            "Rule: ['Jaws (1975)'] -> ['Star Wars: Episode IV - A New Hope (1977)'], Support: 0.23684210526315788, Confidence: 0.8068965517241379\n",
            "Rule: ['Jurassic Park (1993)', 'Princess Bride, The (1987)'] -> ['Star Wars: Episode IV - A New Hope (1977)'], Support: 0.2165991902834008, Confidence: 0.9224137931034483\n",
            "Rule: ['Star Wars: Episode IV - A New Hope (1977)', 'Princess Bride, The (1987)'] -> ['Back to the Future (1985)'], Support: 0.242914979757085, Confidence: 0.8\n",
            "Rule: ['Princess Bride, The (1987)', 'Back to the Future (1985)'] -> ['Star Wars: Episode IV - A New Hope (1977)'], Support: 0.242914979757085, Confidence: 0.851063829787234\n",
            "Rule: ['Jurassic Park (1993)', 'Back to the Future (1985)'] -> ['Star Wars: Episode IV - A New Hope (1977)'], Support: 0.2165991902834008, Confidence: 0.884297520661157\n",
            "Rule: ['Saving Private Ryan (1998)', 'Back to the Future (1985)'] -> ['Star Wars: Episode IV - A New Hope (1977)'], Support: 0.20445344129554655, Confidence: 0.8487394957983193\n",
            "Rule: ['Back to the Future (1985)', \"Schindler's List (1993)\"] -> ['Star Wars: Episode IV - A New Hope (1977)'], Support: 0.20040485829959515, Confidence: 0.8761061946902655\n",
            "Rule: ['Groundhog Day (1993)', 'Princess Bride, The (1987)'] -> ['Back to the Future (1985)'], Support: 0.2125506072874494, Confidence: 0.8076923076923078\n",
            "Rule: ['Saving Private Ryan (1998)', 'Princess Bride, The (1987)'] -> ['Star Wars: Episode IV - A New Hope (1977)'], Support: 0.20647773279352227, Confidence: 0.864406779661017\n",
            "Rule: ['Groundhog Day (1993)', 'Star Wars: Episode IV - A New Hope (1977)'] -> ['Back to the Future (1985)'], Support: 0.22064777327935223, Confidence: 0.8257575757575757\n",
            "Rule: ['Jurassic Park (1993)', 'Saving Private Ryan (1998)'] -> ['Star Wars: Episode IV - A New Hope (1977)'], Support: 0.20040485829959515, Confidence: 0.8250000000000001\n",
            "Rule: ['Godfather, The (1972)', 'Godfather: Part II, The (1974)'] -> ['Star Wars: Episode IV - A New Hope (1977)'], Support: 0.20445344129554655, Confidence: 0.8559322033898304\n",
            "Rule: ['Star Wars: Episode IV - A New Hope (1977)', 'Godfather: Part II, The (1974)'] -> ['Godfather, The (1972)'], Support: 0.20445344129554655, Confidence: 0.9439252336448598\n"
          ]
        }
      ],
      "source": [
        "# Write your code to print out the rules\n",
        "# Generate association rules from frequent itemsets\n",
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    \"\"\"\n",
        "    Generates the association rules from the frequent itemsets.\n",
        "\n",
        "    Args:\n",
        "        frequent_itemsets (list): The list of frequent itemsets.\n",
        "        transactions (list): The list of transactions.\n",
        "        min_confidence (float): The minimum confidence threshold.\n",
        "\n",
        "    Returns:\n",
        "        list: The list of association rules.\n",
        "    \"\"\"\n",
        "    rules = []\n",
        "    for itemset in frequent_itemsets:\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in combinations(itemset, i):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "\n",
        "                # Calculate confidence\n",
        "                antecedent_support = calculate_support(antecedent, transactions)\n",
        "                rule_support = calculate_support(itemset, transactions)\n",
        "                confidence = rule_support / antecedent_support\n",
        "\n",
        "                if confidence >= min_confidence:\n",
        "                    antecedent_movies = get_movie_names(antecedent)\n",
        "                    consequent_movies = get_movie_names(consequent)\n",
        "                    rules.append((antecedent_movies, consequent_movies, rule_support, confidence))\n",
        "\n",
        "    return rules\n",
        "\n",
        "# Apply Apriori algorithm and generate rules\n",
        "rules = generate_rules(frequent_itemsets, transactions, min_confidence=0.8)\n",
        "\n",
        "# Print the association rules\n",
        "for rule in rules:\n",
        "    print(f\"Rule: {rule[0]} -> {rule[1]}, Support: {rule[2]}, Confidence: {rule[3]}\")"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
